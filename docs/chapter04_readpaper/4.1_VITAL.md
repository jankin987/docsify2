# 4.1 VITAL

## 4.1.1 【VITAL(CVPR2018 Spotlight)】阅读笔记
> 出自于https://blog.csdn.net/sinat_31184961/article/details/87639647

论文：VITAL: VIsual Tracking via Adversarial Learning（利用对抗性学习的目标跟踪）
[论文地址](https://arxiv.org/pdf/1804.04273.pdf) [code](https://ybsong00.github.io/cvpr18_tracking/index)

### 写在前面

这篇文章是CVPR2018的Spotlight，感觉18年跟踪方面的Spotlight都是以精度取胜了，不过在跟踪方面还是效果为先，这篇文章的思路并不难，也没有过于复杂的公式推导，可能在一些措辞上有些不好理解。它最大的优点就是把GAN应用到目标跟踪这个领域，相比MDNet效果有了很大的提升，网络也比较简单，沿用了VGG-M的结构，但是由于每帧都要更新模板，所以运行速度也比较慢，大概1.5FPS。

### Motivation

这篇文章指出基于深度学习的目标跟踪存在两个方面的问题：
① 一帧中的正样本之间高度重叠，从这些样本中很难获取目标表观的变化；
② 正负样本数量上的不平衡性。

### Contribution

- 利用对抗学习来加强正样本特征的鲁棒性，使得提取到的特征对目标的表观变化不敏感；
- 利用了一个高阶敏感的损失函数来处理正负样本之间的不平衡
- 在很多的benchmark上验证了这个方法，都获得了比较好的效果。

### Algorithm

这一块分成两部分来讲

#### 一、对抗学习

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190218185504849.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxMTg0OTYx,size_16,color_FFFFFF,t_70)
对于每一帧，先是提取出多个样本，每个样本经过如上网络得到一个分类的score。上图中前面提取特征的网络是VGG-M，在第三个卷积层先是使用了两个全连接层用来随机生成权重的mask，然后再与原先的特征做dropout操作，最后得到的结果到分类器中分类。其中的对抗学习部分就是在mask的生成器G G*G*和分类器D D*D*的对抗。由于传统的GAN的生成器的输入是一堆噪声，而我们有的是proposal，所以才要使用Mask来作为噪声，这样才能将GAN移植过来。

如上图所示，文章将生成器G G*G*放在特征提取和分类之间，假设输出的特征为C C*C*，根据C C*C*生成的MASK是G ( C ) G(C)*G*(*C*)，实际上的MASK为M M*M*，最后损失函数如下：（这里的Mask其实就是充当表观变化的参数，希望学到的判别器对多种表观变化都很鲁棒）
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190218191041209.png)
这个损失函数的含义就是，要让G生成的MASK与C的dropout之后得到的特征让D无法区分，而D的目的就是不断提高自己的判别力，使得能更好的区分G的MASK。值得注意的是，这篇文章对于每个proposal都用了9个Mask，而最终选择的Mask却是使得这个损失函数最大的Mask，然后利用这个值对网络进行优化。文章的解释是，若是一味使用样本中最具有判别力的部分，很容易使得网络陷入过拟合，这种选择最大的方式，其实就是在选一些在表观变化中鲁棒性，不敏感的特征。文章提出这种操作可以一致单个帧中十分具有判别力的特征，得到全局鲁棒性、稳定性的特征。 我个人认为这个地方还存在一些问题，这些被抑制的特征不一定就是某些帧独有的，这么抑制了，可能会使得网络对那些语义相似的噪声难以判别，但是这篇文章的效果很好，可能是我多虑了。也有可能是因为不端选择最难的Mask，其实反倒增强了判别器的判别力。

#### 二、敏感交叉熵损失

这一块主要为了处理正负样本数量不均的问题，引入了focal loss的方式，损失函数：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190218192657657.png)
对于交叉熵的损失函数，正样本时得到的概率越大越好，负样本时得到的概率越小越好，但是由于过多的简单负样本损失叠加，还是会变大，可能会超过一个艰难的负样本的损失，所以以上就是给每个样本加了个权重。艰难的负样本的权重高，简单负样本的权重低。

### Tracking

文章中多次提到他们只在训练阶段使用G，测试阶段只用了D，但是它的跟踪过程是每帧都更新的，这块有点不太理解。

- 模型初始化：线下先预训练模型参数（VGG-M），对于每个视频，先随机初始化D，然后用第一帧训练D和G；
- 在线检测：每来一帧都先提取proposal，然后只做一次前向传播得到分类score；
- 模型更新：利用前一帧得到的结果更新模型，使用G和D，每一帧都更新

### Experiment

实验效果还挺好的，但是还是没超过ECO和CCOT，但是在输入视频分辨率比较低的情况下，表现还不如MDNet：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190218193709287.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxMTg0OTYx,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190218193720533.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxMTg0OTYx,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190218193732663.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxMTg0OTYx,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190218193745978.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxMTg0OTYx,size_16,color_FFFFFF,t_70)

### 总结

可能是看的文章太少了，感觉这篇文章的有些解释部分不太懂，也不怎么理解这篇文章为什么效果好，还需要好好琢磨一下。

#### 优点

- 将GAN引入到目标跟踪领域中来，设计了简单高效的网络
- 使用了focal loss来处理数据不平衡的问题

#### 缺点

- 对输入的测试视频要求高，若其分辨率低效果不如MDNet
- 可能难去区分语义级别的噪声
- 速度很慢，只有1.5FPS

## 4.1.2 目标追踪论文之狼吞虎咽(1):VITAL算法

>出自于https://blog.csdn.net/weixin_38493025/article/details/80674696



参考：https://blog.csdn.net/aiqiu_gogogo/article/details/79982210
代码：https://github.com/ybsong00/Vital_release

### 一、摘要

当前已有的基于神经网络的目标追踪算法大都是基于这样一个两阶段框架：

1. 先是在目标可能出现的位置附近找出一系列样本；
2. 然后对这些样本进行分类，判断是属于目标还是背景。

由此会导致追踪算法在以下两个层面受到限制：

1. 每一帧上的正样本在空间上高度重叠，难以捕获到大规模形变；
2. 类不均衡：正样本数远远小于负样本。

为了解决上述两个问题，作者提出了基于对抗学习的VITAL算法：

1. 为了增强正样本数据，使用一个生成对抗式网络（GAN）随机产生遮罩mask，这些mask作用在输入特征上来捕获目标物体的一系列变化。在对抗学习的作用下，作者的网络能够识别出在整个时序中哪一种mask保留了目标物体的鲁棒性特征。
2. 为了解决类不均衡问题，作者提出了一个高阶的代价敏感损失函数，用于降低简单负样本的影响力，促进网络的训练。

### 二、VITAL算法

#### 2.1 对抗学习

GAN原理：在训练过程中，生成网络G的目标是尽量生成真实的图片去欺骗判别网络D，而D的目标是尽量把G生成的图片和真实的图片区分开来。通过使用标准的交叉熵代价函数，模型的损失函数定义为：

L=minGmaxDV(D,G)L=minGmaxDV(D,G)

V(D,G)=Ex∼Pdata(x)[logD(x)]+Ez∼Pnoise(z)[log(1−D(G(z)))]V(D,G)=Ex∼Pdata(x)[logD(x)]+Ez∼Pnoise(z)[log(1−D(G(z)))]
式(1)

公式说明：

| 符号    | 含义                                 |
| ------- | ------------------------------------ |
| x       | 真实图片                             |
| z       | 随机噪声                             |
| D(x)    | 判别网络D判断真实图片x是否真实的概率 |
| G(z)    | 生成网络G通过随机噪声z生成的伪造图片 |
| D(G(z)) | 判别网络D判断伪造图片是否真实的概率  |

(1)对于判别网络D而言，D(x)越接近于1越好，说明判别网络D明辨是非的能力越强，所以目标是使得V(D,G)变大；
(2)对于生成网络G而言，D(G(z))越接近于1越好，说明生成网络G以假乱真的能力越强，所以目标是使得V(D,G)变小。

> 作者指出传统的GAN并不适合于直接用在目标跟踪任务中，主要有以下三点原因：
> Ⅰ、在目标跟踪中，输入网络的数据不是随机噪声，而是一个从实际图片中采样得到的图像区域块；
> Ⅱ、在目标跟踪中，我们需要有监督的训练分类器，而不是像传统GAN一样做无监督的训练；
> Ⅲ、在目标跟踪中，我们最终的目的是获得分类器D，而不是像传统GAN一样最终目的是获得G。

为了将GANs应用到检测追踪*tracking-by-detection*框架上，
![网络架构](https://img-blog.csdn.net/20180613164209350?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODQ5MzAyNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> 作者在**VGG-M**网络的基础上设计了这样一个网络：在最后一个卷积层和第一个全连接层之间增加了一个生成式网络，从而达到在特征空间增强正样本的目的。具体的，生成式网络输入为目标特征，输出为一个mask矩阵，**该mask矩阵作用于目标特征后表示目标的一种外观变化**。通过对抗学习，该生成式网络可以产生能保留目标特征中最鲁棒部分的mask矩阵（说白了就是自动判断特征中哪部分是目标的鲁棒表达，哪部分对目标变化不鲁棒，干掉后者保留前者的智能mask矩阵）。最终，训练得到的生成式网络生成的mask矩阵可以对判别力强的特征进行削弱，防止判别器过拟合于某个样本。
> （这里，可能会有人提出疑问，削弱有判别力的特征？有没有搞错？当然没有~听我解释：首先，我们要知道判别力强的特征和鲁棒性强的特征是不一样的，打个比方——假设我们要跟踪一个人脸，一开始都是正常的人脸，然后我突然在第100帧的时候往人脸上贴一个小的暴走漫画！那么，对于100帧来说，这个暴走漫画就属于判别力强的特征，因为他相对人脸其他部分来说边缘性强，而且只有人脸这里有这个漫画，其他地方都没有，在第100帧可以合理的认为有漫画的地方就是人脸，这就是判别力强的特征。而什么是鲁棒性强的特征呢？当然是眼睛，眉毛，鼻子，嘴之类的，因为他们始终属于人脸，并且大部分时候都是可见的，我们根据他们来判断一个目标框是不是人脸，从长远角度看是更可靠的，毕竟第150帧你可能撕掉了暴走漫画，但是你撕不掉鼻子吧~所以，我们希望判别器关注鲁棒性强的特征（因为它具有一般性），削弱判别力强的特征（因为它具有偶然性））

网络分成三大部分：
\1. 特征提取器：VGG-M网络提取多通道的特征
\2. 对抗特征生成器：产生能保留目标特征中最鲁棒部分的mask矩阵
\3. 判别分类器：与GAN不同，本文的目的在于获得一个对目标变化鲁棒的判别器。

类似上面的损失函数，这里的损失函数定义为：

LVITAL=minGmaxDV(D,G)LVITAL=minGmaxDV(D,G)

V(D,G)=E(C,M)∼P(C,M)[logD(M∙C)]+EC∼P(C)[log(1−D(G(C)∙C))]+λE(C,M)∼P(C,M)||G(C)−M||2V(D,G)=E(C,M)∼P(C,M)[logD(M∙C)]+EC∼P(C)[log(1−D(G(C)∙C))]+λE(C,M)∼P(C,M)||G(C)−M||2
式(2)

公式说明：

| 符号 | 含义                                                      |
| ---- | --------------------------------------------------------- |
| M    | 在特征C下理论上最优的mask矩阵（让判别器犯错越明显就越优） |
| C    | 目标经过VGG-M网络后得到的多通道的特征                     |
| ∙∙   | 对输入特征C做dropout                                      |
| G(C) | 生成网络G作用于输入特征C生成的mask矩阵                    |

> 上式的训练目的可以描述为：G想得到C后尽力生成mask矩阵，使得D很难判断M⋅C和G(C)⋅C的区别，所以G的目的是生成靠近于M的mask矩阵，于此同时，D又想，即使在C上增加了M⋅C和G(C)⋅C干扰，我也要努力的区分出他们的不同，这样，有干扰的我都不怕，我还会怕你没干扰的吗？明显又是一个G和D相爱相杀的过程；
>
> 网络运行简述：当训练分类器D的时候，提取目标特征C后，不直接将C送给分类器进行训练，而是先使用G网络生成mask，mask作用于C后在送到分类器D中进行分类器学习。值得一提的是，最初G根据C生成随机的mask矩阵（因为初始状态下G是随机的），且每一个mask矩阵都对应目标的一种外观变化，希望多个mask可以尽量覆盖目标的各种变化。通过对抗训练后，G会学会如何根据C产生最难为最不利于分类器D分类的mask（这个时候就很6了），这个时候的G就是一个相对智能的了。当然~不要忘记了我们的D，D发现：我凑~G变聪明了，那我也不能水了，我也要学一些鲁棒的特征来对抗G了~就这样，D和G就既像一对好基友相互促进，又像一对敌人相互为难。

通过上述对抗学习，结果是分类器会更加关注鲁棒的特征，而对有判别力的特征关注较少；为了形象说明，以下图为例：![熵对比](https://img-blog.csdn.net/2018061416295736?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODQ5MzAyNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
对于视频帧（a），不使用对抗学习（GAN）的图（b）的熵分布和（c）很相似。然而，如视频帧（d），当目标经过部分遮挡和平面外旋转后，不使用GAN的图（e）的熵变大，意味着这时候的分类器对目标周围区域的判别变得很模糊了。这是因为训练器训练时专注于历史帧中有判别力的特征，当目标在测试帧中发生变化后，这些有判别力的特征将大大降低分类器的准确度。
而对比图（f），其熵分布就没有像（e）一样变化得很明显了，这是因为分类器是通过上述对抗学习，会更加关注鲁棒的特征，而对有判别力的特征关注较少；

#### 2.2 代价敏感损失

首先让我们一起来回顾一下二分类中交叉熵（cross entropy，简称CE）损失的概念。记y∈∈{0,1}作为类标，p∈∈[0,1]是估计样本是正类（y=1）的概率，则样本是负类（y=0）的概率为1-p。交叉熵损失的公式为：
L(p,y)=−(ylog(p)+(1−y)log(1−p))L(p,y)=−(ylog(p)+(1−y)log(1−p))
交叉熵损失一个显著的问题是简单负样本，例如当p<<0.5且y=0时计算的损失值尽管小，但是如果存在很多简单负样本的话，最后累加起来还是会超过稀缺的正样本的损失值。在目标追踪中，由于正样本少，单简单负样本很多，就会出现类不平衡问题。简单负样本占据了交叉熵损失的绝大部分，主宰着梯度。
目前这个问题的解决方法包括复杂负样本挖掘、训练数据重赋权。想让一个分类器能够对代价敏感，最简单的方法是调整类的重要性。举个栗子，当正负样本数之比为1:100时，则设负类的重要性为0.01。注意到简单地加一个调整因子来平衡正负类的重要性并不能看出每个样本的简单或复杂程度。于是，作者引入了focal loss，在交叉熵损失中根据神经网络输出的概率p加了一个调整因子，改进后的损失函数为：
L(p,y)=−(y(1−p)log(p)+(1−y)plog(1−p))L(p,y)=−(y(1−p)log(p)+(1−y)plog(1−p))
有了上述公式后，作者又重新定义了模型总的误差函数为：

LVITAL=minGmaxDV(D,G)LVITAL=minGmaxDV(D,G)

V(D,G)=E(C,M)∼P(C,M)[K1logD(M∙C)]+EC∼P(C)[K2log(1−D(G(C)∙C))]+λE(C,M)∼P(C,M)||G(C)−M||2V(D,G)=E(C,M)∼P(C,M)[K1logD(M∙C)]+EC∼P(C)[K2log(1−D(G(C)∙C))]+λE(C,M)∼P(C,M)||G(C)−M||2
其中，K1=1−D(M∙C)K1=1−D(M∙C)，K2=D(G(C)∙C)K2=D(G(C)∙C)正是用来平衡训练样本误差的调整因子。

> 那些明明很容易被分类正确的负样本其实在训练过程中也会产生损失，然而不希望网络关注这些损失，因为关注他们反而会使得网络性能变差，实验证明，本文提出的新的损失函数不但可以提升精度，同时可以加速训练的收敛。

## 4.1.3 VITAL: VIsual Tracking via Adversarial Learning论文笔记
> 出自于https://blog.csdn.net/aiqiu_gogogo/article/details/79982210

本文是一篇很不错的关于目标跟踪算法的文章，收录于CVPR2018。

论文链接：https://arxiv.org/pdf/1804.04273.pdf

本文主要分析了现有的检测式跟踪的框架在模型在线学习过程中的两个弊病，即：
①、每一帧中正样本高度重叠，他们无法捕获物体丰富的变化表征；
②、正负样本之间存在严重的不均衡分布的问题；
针对上述问题，本文提出 VITAL 这个算法来解决，主要思路如下：
①、为了丰富正样本，作者采用生成式网络来随机生成mask，且这些mask作用在输入特征上来捕获目标物体的一系列变化。在对抗学习的作用下，作者的网络能够识别出在整个时序中哪一种mask保留了目标物体的鲁邦性特征；
②、在解决正负样本不均衡的问题中，本文提出了一个高阶敏感损失来减小简单负样本对于分类器训练的影响。

个人评价：本文思路明确，解决问题的方法新颖且有效，实验效果好，不愧是通过残酷的CVPR2018筛选后的精品文章！

### GAN

①、既然本文是基于GAN思想的一篇文章，然而考虑到有些读者只是听说过GAN怎么怎么火，怎么怎么牛，其实并不太了解GAN是个什么东西，我就尽量简单通俗的说一下我对GAN思想的理解，用来理解这篇文章应该是够了（我也是现学现卖~为了看懂本文，特意去看了一下什么是GAN，已经懂GAN的大神请自动飘过这一段~~~）。

②、什么是GAN？
首先，假设我们有一个非常先进的测谎仪，这个测谎仪很强大，我们一说谎，它就能“哔~”一声，然而它不是完美的，他也是有漏洞的，只是我们难以发现。现在，我们的目标是做一个“说谎仪”，虽然我们本人没办法骗过测谎仪，但是我们可以通过训练一个说谎仪来说谎，并且希望这个说谎仪的谎话能骗过测谎仪就OK了。既然测谎仪很强大，那么我们在训练过程中就使用测谎仪，说谎仪没骗过测谎仪我们就fine-tune说谎仪，直到它战胜了测谎仪为止。
上述说谎仪和测谎仪博弈的过程就是GAN的主要思想。GAN有两个组件，分别为：生成器和判别器，这里，生成器就相当于刚刚说的说谎仪，判别器就相当于测谎仪。我们训练GAN的主要目的就是想在判别器足够强大的前提下，训练生成器让判别器认为生成器生成的样本就是“真”样本，也就是让说谎仪说一句谎话，希望测谎仪误认为这是真话，那么我们认为这个生成器（说谎仪）就训练成功或者说足够强大了。

③、一般GAN的数学表达：
![这里写图片描述](https://img-blog.csdn.net/20180418092204522?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FpcWl1X2dvZ29nbw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)（不要一看到公式就怂哈！放轻松，不复杂的~）
符号解释：分布的真实样本；
解释：上述公式是一个损失函数，如果觉得就像太极中黑和白一样，相爱相杀，相互促进，谁也不服谁；
训练目的：再强也没什么办法了~~~；
结果：训练结束后，我们扔掉；

④、作者指出传统的GAN并不适合于直接用在目标跟踪任务中，主要有以下三点原因：
Ⅰ、在目标跟踪中，输入网络的数据不是随机噪声，而是一个从实际图片中采样得到的图像区域块；
Ⅱ、在目标跟踪中，我们需要有监督的训练分类器，而不是像传统GAN一样做无监督的训练；
Ⅲ、在目标跟踪中，我们最终的目的是获得分类器。

### Motivation

①、由于基于检测式跟踪的框架存在每一帧中正样本高度重叠的现象，所以他们无法捕获物体丰富的变化表征，之前的基于深度学习的跟踪器们在丰富训练样本多样性上突出的工作较少。一般来说，分类器在学习过程中更加关注距离分类面较近的样本，也就是更具有判别力的样本。然而，（**※※※INSIGHT※※※**）在目标跟踪中，目标在各帧之间变化迥异，在当前帧认为最有判别力的样本，在后续帧中未必是最有判别力的，所以用当前帧训练的模型在后续帧中泛化能力可能较差，因此一些目标短暂的的部分遮挡或者平面外旋转可能造成模型更新漂移。所以，如何在特征空间中对正样本进行增强，从而在丰富目标的变化以更好更鲁棒的更新跟踪器模型，是跟踪器设计过程中的重要问题。（这一段不知道说明白没有~）

②、在目标跟踪问题的模型更新过程中存在明显的正负样本比例失调的问题（这个很好理解），因此如何使得跟踪器的更新更关注有判别力的样本，削弱那些很简单的负样本对跟踪器更新的影响，对跟踪器的鲁棒性至关重要！

### 本文方法概述

①，本文在VGG-M模型基础上进行改进，在最后一个卷积层和第一个全连接层之间增加了一个产生式网络，从而达到在特征空间增强正样本的目的。具体的，产生式网络输入为目标特征，输出为一个mask矩阵，该mask矩阵作用于目标特征后表示目标的一种外观变化。通过对抗学习，该产生式网络可以产生能保留目标特征中最鲁棒部分的mask矩阵（说白了就是自动判断特征中哪部分是目标的鲁棒表达，哪部分对目标变化不鲁棒，干掉后者保留前者的智能mask矩阵）。最终，训练得到的产生式网络生成的mask矩阵可以对判别力强的特征进行削弱，防止判别器过拟合于某个样本。
（这里，可能会有人提出疑问，削弱有判别力的特征？有没有搞错？当然没有~听我解释：首先，我们要知道判别力强的特征和鲁棒性强的特征是不一样的，打个比方——假设我们要跟踪一个人脸，一开始都是正常的人脸，然后我突然在第100帧的时候往人脸上贴一个小的暴走漫画！那么，对于100帧来说，这个暴走漫画就属于判别力强的特征，因为他相对人脸其他部分来说边缘性强，而且只有人脸这里有这个漫画，其他地方都没有，在第100帧可以合理的认为有漫画的地方就是人脸，这就是判别力强的特征。而什么是鲁棒性强的特征呢？当然是眼睛，眉毛，鼻子，嘴之类的，因为他们始终属于人脸，并且大部分时候都是可见的，我们根据他们来判断一个目标框是不是人脸，从长远角度看是更可靠的，毕竟第150帧你可能撕掉了暴走漫画，但是你撕不掉鼻子吧~所以，我们希望判别器关注鲁棒性强的特征（因为它具有一般性），削弱判别力强的特征（因为它具有偶然性））；

②、本文提出了一个高阶敏感损失来减小简单负样本对于分类器训练的影响，这个道理很简单，那些明明很容易被分类正确的负样本其实在训练过程中也会产生损失，然而不希望网络关注这些损失，因为关注他们反而会使得网络性能变差，实验证明，本文提出的新的损失函数不但可以提升精度，同时可以加速训练的收敛。

### 本文方法——对抗学习in目标跟踪

①、先上网络结构图吧：
![这里写图片描述](https://img-blog.csdn.net/20180418091203263?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FpcWl1X2dvZ29nbw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

②、在解释网络结构之前先翻译几句作者对方法的重要阐述：
Ⅰ、我们将分类层看作为一个判别器，并且提出了一个用来进行对抗学习的生成器；
Ⅱ、一般来说，已有的GAN都是目的得到一个生成器，用来将一个随机分布转换为一个指定分布，不像已有的GAN方法，本文的目的在于获得一个对目标变化鲁棒的判别器；

③、网络简析：在特征提取和分类器之间增加了一个生成式网络，被用来产生加权的作用于目标的特征的mask矩阵（目的就是为了在特征层面丰富目标的多样性），mask矩阵为单通道的和特征分辨率相同的矩阵，与特征通道做点乘操作；

④、本文核心损失函数（坚持一下，快结束了）：
![这里写图片描述](https://img-blog.csdn.net/20180418103739492?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FpcWl1X2dvZ29nbw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
符号解析：好坏啊~~~）；
公式解析：上式的训练目的可以描述为：相爱相杀的过程；

⑤、网络运行简述：当训练分类器就既像一对好基友相互促进，又像一对敌人相互为难。

⑥、训练：参考⑤中描述；

⑦、训练就可以训练④中损失啦；

⑧、再次强调：通过上述对抗学习，结果是分类器会更加关注鲁棒的特征，而对有判别力的特征关注较少；

⑨、（行啦行啦，完事儿啦~后面的仅供参观，不费脑筋了）
上个图欣赏一下本文作者对抗学习的成果：
![这里写图片描述](https://img-blog.csdn.net/20180418144305763?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FpcWl1X2dvZ29nbw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
在这个图中，偏向蓝色表示分类器很确定这里是啥，偏向红色表示分类器有点懵。从第二列可以看出，不经过对抗学习的结果，当这个脸转动了一下以后，分类器表示大部分地方他都很迷惑，相比之下，第三列可以看出，经过对抗学习的结果，即使目标平面外旋转了一下，分类器相比不经过对抗学习的结果更加确信目标区域是什么！

⑩、没了~损失函数太简单，不是本文的精髓~不讲

### 运行细节

①、模型初始化：分两阶段
Ⅰ、In the first step we offline pretrain the model using positive and negative samples from the training data, which is from MDNet——额~说实话，这句话我没读懂啊~借助MDNet的力量？？？MDNet当前貌似被认为是犯规的力量啊~
Ⅱ、使用第一帧fine-tune网络——这是个常规操作；

②、在线监测：这里没啥好说的，一般基于检测的跟踪框架怎么做他就怎么做，唯一值得注意的是，检测过程中污染我的检测样本，是要闹哪样？

③、在线更新：没啥好说的，从前一帧采集样本，相互玩耍~

### 实验

①，OTB2013和OTB2015结果，去年还说ECO已经把这两个库做到了极致，现在有对手了：

![这里写图片描述](https://img-blog.csdn.net/20180418145050851?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FpcWl1X2dvZ29nbw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![这里写图片描述](https://img-blog.csdn.net/20180418145147799?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FpcWl1X2dvZ29nbw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

②、速度：1.5fps，这还是在Tesla K40c GPU下（腾讯有钱啊！），幸亏没有实时，否则目标跟踪到此为止了~~~；

③、VOT2016结果：
![这里写图片描述](https://img-blog.csdn.net/20180418145322654?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FpcWl1X2dvZ29nbw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
太好了，没超过ECO和CCOT，否则做跟踪的可以考虑转行了；

### 本文概览

本文是一篇很不错的关于目标跟踪算法的文章，收录于CVPR2018。

论文链接：https://arxiv.org/pdf/1804.04273.pdf

本文主要分析了现有的检测式跟踪的框架在模型在线学习过程中的两个弊病，即：
①、每一帧中正样本高度重叠，他们无法捕获物体丰富的变化表征；
②、正负样本之间存在严重的不均衡分布的问题；
针对上述问题，本文提出 VITAL 这个算法来解决，主要思路如下：
①、为了丰富正样本，作者采用生成式网络来随机生成mask，且这些mask作用在输入特征上来捕获目标物体的一系列变化。在对抗学习的作用下，作者的网络能够识别出在整个时序中哪一种mask保留了目标物体的鲁邦性特征；
②、在解决正负样本不均衡的问题中，本文提出了一个高阶敏感损失来减小简单负样本对于分类器训练的影响。

个人评价：本文思路明确，解决问题的方法新颖且有效，实验效果好，不愧是通过残酷的CVPR2018筛选后的精品文章！

### GAN

①、既然本文是基于GAN思想的一篇文章，然而考虑到有些读者只是听说过GAN怎么怎么火，怎么怎么牛，其实并不太了解GAN是个什么东西，我就尽量简单通俗的说一下我对GAN思想的理解，用来理解这篇文章应该是够了（我也是现学现卖~为了看懂本文，特意去看了一下什么是GAN，已经懂GAN的大神请自动飘过这一段~~~）。

②、什么是GAN？
首先，假设我们有一个非常先进的测谎仪，这个测谎仪很强大，我们一说谎，它就能“哔~”一声，然而它不是完美的，他也是有漏洞的，只是我们难以发现。现在，我们的目标是做一个“说谎仪”，虽然我们本人没办法骗过测谎仪，但是我们可以通过训练一个说谎仪来说谎，并且希望这个说谎仪的谎话能骗过测谎仪就OK了。既然测谎仪很强大，那么我们在训练过程中就使用测谎仪，说谎仪没骗过测谎仪我们就fine-tune说谎仪，直到它战胜了测谎仪为止。
上述说谎仪和测谎仪博弈的过程就是GAN的主要思想。GAN有两个组件，分别为：生成器和判别器，这里，生成器就相当于刚刚说的说谎仪，判别器就相当于测谎仪。我们训练GAN的主要目的就是想在判别器足够强大的前提下，训练生成器让判别器认为生成器生成的样本就是“真”样本，也就是让说谎仪说一句谎话，希望测谎仪误认为这是真话，那么我们认为这个生成器（说谎仪）就训练成功或者说足够强大了。

③、一般GAN的数学表达：
![这里写图片描述](https://img-blog.csdn.net/20180418092204522?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FpcWl1X2dvZ29nbw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)（不要一看到公式就怂哈！放轻松，不复杂的~）
符号解释：分布的真实样本；
解释：上述公式是一个损失函数，如果觉得就像太极中黑和白一样，相爱相杀，相互促进，谁也不服谁；
训练目的：再强也没什么办法了~~~；
结果：训练结束后，我们扔掉；

④、作者指出传统的GAN并不适合于直接用在目标跟踪任务中，主要有以下三点原因：
Ⅰ、在目标跟踪中，输入网络的数据不是随机噪声，而是一个从实际图片中采样得到的图像区域块；
Ⅱ、在目标跟踪中，我们需要有监督的训练分类器，而不是像传统GAN一样做无监督的训练；
Ⅲ、在目标跟踪中，我们最终的目的是获得分类器。

### Motivation

①、由于基于检测式跟踪的框架存在每一帧中正样本高度重叠的现象，所以他们无法捕获物体丰富的变化表征，之前的基于深度学习的跟踪器们在丰富训练样本多样性上突出的工作较少。一般来说，分类器在学习过程中更加关注距离分类面较近的样本，也就是更具有判别力的样本。然而，（**※※※INSIGHT※※※**）在目标跟踪中，目标在各帧之间变化迥异，在当前帧认为最有判别力的样本，在后续帧中未必是最有判别力的，所以用当前帧训练的模型在后续帧中泛化能力可能较差，因此一些目标短暂的的部分遮挡或者平面外旋转可能造成模型更新漂移。所以，如何在特征空间中对正样本进行增强，从而在丰富目标的变化以更好更鲁棒的更新跟踪器模型，是跟踪器设计过程中的重要问题。（这一段不知道说明白没有~）

②、在目标跟踪问题的模型更新过程中存在明显的正负样本比例失调的问题（这个很好理解），因此如何使得跟踪器的更新更关注有判别力的样本，削弱那些很简单的负样本对跟踪器更新的影响，对跟踪器的鲁棒性至关重要！

### 本文方法概述

①，本文在VGG-M模型基础上进行改进，在最后一个卷积层和第一个全连接层之间增加了一个产生式网络，从而达到在特征空间增强正样本的目的。具体的，产生式网络输入为目标特征，输出为一个mask矩阵，该mask矩阵作用于目标特征后表示目标的一种外观变化。通过对抗学习，该产生式网络可以产生能保留目标特征中最鲁棒部分的mask矩阵（说白了就是自动判断特征中哪部分是目标的鲁棒表达，哪部分对目标变化不鲁棒，干掉后者保留前者的智能mask矩阵）。最终，训练得到的产生式网络生成的mask矩阵可以对判别力强的特征进行削弱，防止判别器过拟合于某个样本。
（这里，可能会有人提出疑问，削弱有判别力的特征？有没有搞错？当然没有~听我解释：首先，我们要知道判别力强的特征和鲁棒性强的特征是不一样的，打个比方——假设我们要跟踪一个人脸，一开始都是正常的人脸，然后我突然在第100帧的时候往人脸上贴一个小的暴走漫画！那么，对于100帧来说，这个暴走漫画就属于判别力强的特征，因为他相对人脸其他部分来说边缘性强，而且只有人脸这里有这个漫画，其他地方都没有，在第100帧可以合理的认为有漫画的地方就是人脸，这就是判别力强的特征。而什么是鲁棒性强的特征呢？当然是眼睛，眉毛，鼻子，嘴之类的，因为他们始终属于人脸，并且大部分时候都是可见的，我们根据他们来判断一个目标框是不是人脸，从长远角度看是更可靠的，毕竟第150帧你可能撕掉了暴走漫画，但是你撕不掉鼻子吧~所以，我们希望判别器关注鲁棒性强的特征（因为它具有一般性），削弱判别力强的特征（因为它具有偶然性））；

②、本文提出了一个高阶敏感损失来减小简单负样本对于分类器训练的影响，这个道理很简单，那些明明很容易被分类正确的负样本其实在训练过程中也会产生损失，然而不希望网络关注这些损失，因为关注他们反而会使得网络性能变差，实验证明，本文提出的新的损失函数不但可以提升精度，同时可以加速训练的收敛。

### 本文方法——对抗学习in目标跟踪

①、先上网络结构图吧：
![这里写图片描述](https://img-blog.csdn.net/20180418091203263?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FpcWl1X2dvZ29nbw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

②、在解释网络结构之前先翻译几句作者对方法的重要阐述：
Ⅰ、我们将分类层看作为一个判别器，并且提出了一个用来进行对抗学习的生成器；
Ⅱ、一般来说，已有的GAN都是目的得到一个生成器，用来将一个随机分布转换为一个指定分布，不像已有的GAN方法，本文的目的在于获得一个对目标变化鲁棒的判别器；

③、网络简析：在特征提取和分类器之间增加了一个生成式网络，被用来产生加权的作用于目标的特征的mask矩阵（目的就是为了在特征层面丰富目标的多样性），mask矩阵为单通道的和特征分辨率相同的矩阵，与特征通道做点乘操作；

④、本文核心损失函数（坚持一下，快结束了）：
![这里写图片描述](https://img-blog.csdn.net/20180418103739492?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FpcWl1X2dvZ29nbw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
符号解析：好坏啊~~~）；
公式解析：上式的训练目的可以描述为：相爱相杀的过程；

⑤、网络运行简述：当训练分类器就既像一对好基友相互促进，又像一对敌人相互为难。

⑥、训练：参考⑤中描述；

⑦、训练就可以训练④中损失啦；

⑧、再次强调：通过上述对抗学习，结果是分类器会更加关注鲁棒的特征，而对有判别力的特征关注较少；

⑨、（行啦行啦，完事儿啦~后面的仅供参观，不费脑筋了）
上个图欣赏一下本文作者对抗学习的成果：
![这里写图片描述](https://img-blog.csdn.net/20180418144305763?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FpcWl1X2dvZ29nbw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
在这个图中，偏向蓝色表示分类器很确定这里是啥，偏向红色表示分类器有点懵。从第二列可以看出，不经过对抗学习的结果，当这个脸转动了一下以后，分类器表示大部分地方他都很迷惑，相比之下，第三列可以看出，经过对抗学习的结果，即使目标平面外旋转了一下，分类器相比不经过对抗学习的结果更加确信目标区域是什么！

⑩、没了~损失函数太简单，不是本文的精髓~不讲

### 运行细节

①、模型初始化：分两阶段
Ⅰ、In the first step we offline pretrain the model using positive and negative samples from the training data, which is from MDNet——额~说实话，这句话我没读懂啊~借助MDNet的力量？？？MDNet当前貌似被认为是犯规的力量啊~
Ⅱ、使用第一帧fine-tune网络——这是个常规操作；

②、在线监测：这里没啥好说的，一般基于检测的跟踪框架怎么做他就怎么做，唯一值得注意的是，检测过程中污染我的检测样本，是要闹哪样？

③、在线更新：没啥好说的，从前一帧采集样本，相互玩耍~

### 实验

①，OTB2013和OTB2015结果，去年还说ECO已经把这两个库做到了极致，现在有对手了：

![这里写图片描述](https://img-blog.csdn.net/20180418145050851?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FpcWl1X2dvZ29nbw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![这里写图片描述](https://img-blog.csdn.net/20180418145147799?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FpcWl1X2dvZ29nbw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

②、速度：1.5fps，这还是在Tesla K40c GPU下（腾讯有钱啊！），幸亏没有实时，否则目标跟踪到此为止了~~~；



③、VOT2016结果：
![这里写图片描述](https://img-blog.csdn.net/20180418145322654?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FpcWl1X2dvZ29nbw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
太好了，没超过ECO和CCOT，否则做跟踪的可以考虑转行了；

