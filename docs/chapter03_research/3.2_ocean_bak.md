
## 实验的整理
### 准备

**1.Trackit**（主要使用，数据集直接放在固态）
电脑：cv@supermicro
代码路径：`/mnt/data/tracking/wu_m/TracKit`或`/home/cv/wu/TracKit`
conda环境：Trackit

**2. BottleneckTransformers**（botnet的代码参考）
电脑：gby-wm@docker
代码路径：`/root/wu/github/BottleneckTransformers`
conda环境：pytracking2

**3. 电脑：gby-wm@docker**（备用）
代码路径：`/root/wu/TracKit`
conda环境：Trackit2

**4. 电脑：gby-wm@docker**（备用）
代码路径：`/root/wu/wyc/SOTS`
conda环境：Trackit1
### 代码的整理

#### 第一次修改，调试成功
**lib/models/connect.py**
- 添加`from .dcn.deform_conv import DeformConv`

#### 第二次修改，插入botnet

**lib/models/ocean.py**
- 修改Ocean_类，加入mhsa和single的特征提取

**lib/models/models.py**
- Ocean类添加self.feature_mhsa和self.feature_single

**lib/models/backbones.py**
- 添加ResNet_MHSA, ResNet_single类
- Resnet50类中6改为5

**lib/models/modules.py**
- 添加MHSA，Botteneck_MHSA, Resnet_plus_MHSA, Resnet_plus_single类

## 调试
### 调试BottleneckTransformers得出resnet和botnet基础模型
botnet

![image-20210222124147532](C:\Users\15154\AppData\Roaming\Typora\typora-user-images\image-20210222124147532.png)

botnet结构
```
self.query:  1*1, 512,s1
self.key: 1*1, 512,s1
self.value: 1*1, 512, s1
self.rel_h: 1,512,1,14
self.rel_w: 1,512,14,1

x: 2,512,14,14   n_batch,c,w,h
q=self.query(x).view(2,512,-1)   [2,512,14,14]-->[2,512,196]
k=self.key(x).view(2,512,-1)   [2,512,14,14]-->[2,512,196]
v=self.value(x).view(2,512,-1)   [2,512,14,14]-->[2,512,196]

content=q*k   [2,196,196]
position=(self.rel_h+self.rel_w).view(1,C,-1).permute(0,2,1)
          [1,512,14,14]           [1,512,196]    [1,196,512]
position=position*q   [2,196,196]
energy=content+position
attention: [2,196,196]

out: v*attention(0,2,1)  [2,512,196]
out=out.view   [2,512,14,14]
```

### Ocean中backbone结构

![image-20210222150528993](C:\Users\15154\AppData\Roaming\Typora\typora-user-images\image-20210222150528993.png)

### 类siam_tracker属性：
siam
Args:
align=True,arch=ocean,dataset=vot2018,epoch_test=false,online=false

resume='snapshot/oceanV' video=none

Info:TRT:FALSE. Arch:ocean. Dataset:vot2018.  Epoch_test:false

Info:TRT:FALSE. Arch:ocean. Dataset:vot2018.  Epoch_test:false,online:false

Siam_info.align=true

self.stride=8

Self.align=true

self.online=false

Self.trt=false

### 实验

#### 第一次实验，制作base

日志

`Ocean_2021-02-02-00-14`:第一次训练，训练了10个小时，完成了15%，batch使用的32

`Ocean_2021-02-02-10-12`：测试，读取checkpoint3，其他没变

`Ocean_2021-02-02-10-17`：测试，读取checkpoint3，batch使用64



`Ocean_2021-02-02-10-25`：读取原始预训练网络，batch使用64，训了25分钟



`Ocean_2021-02-02-10-52`：读取原始预训练网络，batch使用32，和第一次的配置一样，现在一直在训练

查看日志：
`~/wu/TracKit/logs/Ocean# tensorboard --logdir=./Ocean_2021-02-02-10-52 --port=5901`



几个训练数据集的大小：
vid:82G
det:18G
coco: 56G
got10k: 67G
y2b 144G

![img](file:///C:\Users\15154\Documents\Tencent Files\1515469795\Image\C2C\BXYDU8V1S[18{1{]7ADECFV.png)

Ocean base训练完后
把online和OA分支都去掉了做一个base，三张卡训20个epoch训了45个小时，损失函数是上面那样的。训完后用checkpoint_e20.pth在OTB2015上测的success是0.672，precision是0.898，下一步准备把backbone换vit再试

测一下5-20个epoch的结果

checkpoint_e20的结果存放：`/root/wu/TracKit/result/OTB2015/Oceanbase_checkpoint_e20`
OTB2015 success：0.672  precision:0.898

Oceanbase_checkpoint_e19
OTB2015 success：0.672  precision:0.897

| checkpoint               | success | precision |
| ------------------------ | ------- | --------- |
| Oceanbase_checkpoint_e20 | 0.672   | 0.898     |
| Oceanbase_checkpoint_e19 | 0.672   | 0.897     |
| Oceanbase_checkpoint_e18 | 0.668   | 0.896     |
| Oceanbase_checkpoint_e17 | 0.661   | 0.882     |
| Oceanbase_checkpoint_e16 | 0.668   | 0.894     |
| Oceanbase_checkpoint_e15 | 0.662   | 0.886     |
| Oceanbase_checkpoint_e14 | 0.670   | 0.896     |
| Oceanbase_checkpoint_e13 | 0.662   | 0.881     |
| Oceanbase_checkpoint_e12 | 0.667   | 0.889     |
| Oceanbase_checkpoint_e11 | 0.649   | 0.862     |
| Oceanbase_checkpoint_e10 | 0.634   | 0.850     |
| Oceanbase_checkpoint_e9  | 0.622   | 0.839     |
| Oceanbase_checkpoint_e8  | 0.633   | 0.850     |
| Oceanbase_checkpoint_e7  | 0.628   | 0.845     |
| Oceanbase_checkpoint_e6  | 0.609   | 0.834     |
| Oceanbase_checkpoint_e5  | 0.565   | 0.788     |



test：
```
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e20.pth --dataset OTB2015 --align False
```

`python lib/eval_toolkit/bin/eval.py --dataset_dir dataset/OTB2015 --dataset OTB2015 --tracker_result_dir result/OTB2015 --trackers Ocean`

我们把trackit跑通后，由于需要把数据放到2T的固态硬盘上，所以对docker做了一次commit，commit前主盘的容量272，占比66%，commit后容量292占比70%。足足多了20G，

这里，不得不考虑直接将trackit放到主盘来做
以后，当commit前尽量将无用的主盘数据删干净
尽量每个conda环境单独做docker

python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e17.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e17
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e16.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e16
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e15.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e15
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e14.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e14
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e13.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e13
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e12.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e12
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e11.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e11
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e10.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e10
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e9.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e9
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e8.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e8
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e7.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e7
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e6.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e6
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e5.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e5



#### 第二次实验，加入botnet看效果

### 调试的一些操作

for i in trainable_params[0]['params']:
    print(i)

test=[filter(lambda x: x.requires_grad,model.features_mhsa_z.features_mhsa.parameters())]

test
[<filter object at 0x...118c32350>]
for i in test[0]:
    print(i)  打印出一堆参数



model.features.features.parameters()
<generator object Module.parameters at 0x7fc118bc9a50>
for i in model.features.features.parameters():
    print(i) 打印出一堆参数



for i in model.features.features.parameters():
    print(i.requires_grad)

代码执行流程：

```python
#self.features = ResNet50(used_layers=[3], online=online)  
class ResNet50(nn.Module):
    def __init__(self, used_layers=[2, 3, 4], online=False):
        super(ResNet50, self).__init__()
        self.features = ResNet_plus2(Bottleneck, [3, 4, 6, 3], used_layers=used_layers, online=online)

    def forward(self, x, online=False,templ=None):
        if not online:
            x_stages, x = self.features(x, online=online,templ=templ)
            return x_stages, x
        else:
            x = self.features(x, online=online)
            return x
```

```python
class ResNet_plus2(nn.Module):
    def __init__(self, block, layers, used_layers, online=False):
        self.inplanes = 64
        super(ResNet_plus2, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=0,  # 3
                               bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)

        self.feature_size = 128 * block.expansion
        self.used_layers = used_layers
        self.layer3_use = True if 3 in used_layers else False
        self.layer4_use = True if 4 in used_layers else False

        if self.layer3_use:
            if online:
                self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2, update=True)
                self.layeronline = self._make_layer(block, 256, layers[2], stride=2)
            else:
                self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)

            self.feature_size = (256 + 128) * block.expansion
        else:
            self.layer3 = lambda x: x  # identity

        if self.layer4_use:
            self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)  # 7x7, 3x3
            self.feature_size = 512 * block.expansion
        else:
            self.layer4 = lambda x: x  # identity

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, update=False):
        downsample = None
        dd = dilation
        if stride != 1 or self.inplanes != planes * block.expansion:
            if stride == 1 and dilation == 1:
                downsample = nn.Sequential(
                    nn.Conv2d(self.inplanes, planes * block.expansion,
                              kernel_size=1, stride=stride, bias=False),
                    nn.BatchNorm2d(planes * block.expansion),
                )
            else:
                if dilation > 1:
                    dd = dilation // 2
                    padding = dd
                else:
                    dd = 1
                    padding = 0
                downsample = nn.Sequential(
                    nn.Conv2d(self.inplanes, planes * block.expansion,
                              kernel_size=3, stride=stride, bias=False,
                              padding=padding, dilation=dd),
                    nn.BatchNorm2d(planes * block.expansion),
                )

        layers = []
        layers.append(block(self.inplanes, planes, stride=stride,
                            downsample=downsample, dilation=dilation))
        self.inplanes = planes * block.expansion
        if blocks!=6:
            for i in range(1, blocks):
                layers.append(block(self.inplanes, planes, dilation=dilation))
        else:
            for i in range(1, blocks-1):
                layers.append(block(self.inplanes, planes, dilation=dilation))
            #layers.append(block(self.inplanes, planes, dilation=dilation,mhsa=True))
            layers.append(block(self.inplanes, planes, dilation=dilation))

        if update: self.inplanes = int(self.inplanes / 2)  # for online
        return nn.Sequential(*layers)

    def forward(self, x, online=False):
        x = self.conv1(x)
        x = self.bn1(x)
        x_ = self.relu(x)
        x = self.maxpool(x_)

        p1 = self.layer1(x)
        p2 = self.layer2(p1)

        if online: return self.layeronline(p2)
        p3 = self.layer3(p2)

        return [x_, p1, p2], p3
```

```python
class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1, mhsa=False,templ=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        padding = 2 - stride
        if downsample is not None and dilation > 1:
            dilation = dilation // 2
            padding = dilation

        assert stride == 1 or dilation == 1, \
            "stride and dilation must have one equals to zero at least"

        if dilation > 1:
            padding = dilation
        if not mhsa:
            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
                               padding=padding, bias=False, dilation=dilation)
        else:
            self.conv2 = nn.Sequential(
                    MHSA(planes)
                    # MHSA(planes, width=4, height=4), # for CIFAR10
                )
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * 4)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x, templ=None):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        if templ is None:
            out = self.conv2(out)
        else:
            out = self.conv2(out,templ=templ)


        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual

        out = self.relu(out)

        return out
```

```python
# self.neck = AdjustLayer(in_channels=1024, out_channels=256)
class AdjustLayer(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(AdjustLayer, self).__init__()
        self.downsample = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_channels),
            )

    def forward(self, x, crop=False):
        x_ori = self.downsample(x)
        if x_ori.size(3) < 20 and crop:
            l = 4
            r = -4
            xf = x_ori[:, :, l:r, l:r]

        if not crop:
            return x_ori
        else:
            return x_ori, xf
```

```python
# self.connect_model = box_tower(inchannels=256, outchannels=256, towernum=4)
class box_tower(nn.Module):
    """
    box tower for FCOS reg
    """
    def __init__(self, inchannels=512, outchannels=256, towernum=1):
        super(box_tower, self).__init__()
        tower = []
        cls_tower = []
        # encode backbone
        self.cls_encode = matrix(in_channels=inchannels, out_channels=outchannels)
        self.reg_encode = matrix(in_channels=inchannels, out_channels=outchannels)
        self.cls_dw = GroupDW(in_channels=inchannels)
        self.reg_dw = GroupDW(in_channels=inchannels)

        # box pred head
        for i in range(towernum):
            if i == 0:
                tower.append(nn.Conv2d(outchannels, outchannels, kernel_size=3, stride=1, padding=1))
            else:
                tower.append(nn.Conv2d(outchannels, outchannels, kernel_size=3, stride=1, padding=1))

            tower.append(nn.BatchNorm2d(outchannels))
            tower.append(nn.ReLU())

        # cls tower
        for i in range(towernum):
            if i == 0:
                cls_tower.append(nn.Conv2d(outchannels, outchannels, kernel_size=3, stride=1, padding=1))
            else:
                cls_tower.append(nn.Conv2d(outchannels, outchannels, kernel_size=3, stride=1, padding=1))

            cls_tower.append(nn.BatchNorm2d(outchannels))
            cls_tower.append(nn.ReLU())

        self.add_module('bbox_tower', nn.Sequential(*tower))
        self.add_module('cls_tower', nn.Sequential(*cls_tower))


        # reg head
        self.bbox_pred = nn.Conv2d(outchannels, 4, kernel_size=3, stride=1, padding=1)
        self.cls_pred = nn.Conv2d(outchannels, 1, kernel_size=3, stride=1, padding=1)

        # adjust scale
        self.adjust = nn.Parameter(0.1 * torch.ones(1))
        self.bias = nn.Parameter(torch.Tensor(1.0 * torch.ones(1, 4, 1, 1)).cuda())

    def forward(self, search, kernal, update=None):
        # encode first
        if update is None:
            cls_z, cls_x = self.cls_encode(kernal, search)   # [z11, z12, z13]
        else:
            cls_z, cls_x = self.cls_encode(update, search)  # [z11, z12, z13]

        reg_z, reg_x = self.reg_encode(kernal, search)  # [x11, x12, x13]

        # cls and reg DW
        cls_dw = self.cls_dw(cls_z, cls_x)
        reg_dw = self.reg_dw(reg_z, reg_x)
        x_reg = self.bbox_tower(reg_dw)
        x = self.adjust * self.bbox_pred(x_reg) + self.bias
        x = torch.exp(x)

        # cls tower
        c = self.cls_tower(cls_dw)
        cls = 0.1 * self.cls_pred(c)

        return x, cls, cls_dw, x_reg
```

```python
class matrix(nn.Module):
    """
    encode backbone feature
    """
    def __init__(self, in_channels, out_channels):
        super(matrix, self).__init__()

        # same size (11)
        self.matrix11_k = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )
        self.matrix11_s = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )

        # h/2, w
        self.matrix12_k = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, kernel_size=3, bias=False, dilation=(2, 1)),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )
        self.matrix12_s = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, kernel_size=3, bias=False, dilation=(2, 1)),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )

        # w/2, h
        self.matrix21_k = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, kernel_size=3, bias=False, dilation=(1, 2)),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )
        self.matrix21_s = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, kernel_size=3, bias=False, dilation=(1, 2)),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )

    def forward(self, z, x):
        z11 = self.matrix11_k(z)
        x11 = self.matrix11_s(x)

        z12 = self.matrix12_k(z)
        x12 = self.matrix12_s(x)

        z21 = self.matrix21_k(z)
        x21 = self.matrix21_s(x)

        return [z11, z12, z21], [x11, x12, x21]
```

```python
class GroupDW(nn.Module):
    """
    encode backbone feature
    """
    def __init__(self, in_channels=256):
        super(GroupDW, self).__init__()
        self.weight = nn.Parameter(torch.ones(3))

    def forward(self, z, x):
        z11, z12, z21 = z
        x11, x12, x21 = x

        re11 = xcorr_depthwise(x11, z11)
        re12 = xcorr_depthwise(x12, z12)
        re21 = xcorr_depthwise(x21, z21)
        re = [re11, re12, re21]
        
        # weight
        weight = F.softmax(self.weight, 0)

        s = 0
        for i in range(3):
            s += weight[i] * re[i]

        return s
```

