第四次实验：

gfocal

train.py

mmdet/models/anchor_heads/gfl_head.py

```
class GFLHead
num_classes=81
in_channels=256
stacked_conv=4
octavebasescale=8
scalse_per_octave=1
norm_cfg=GN
loss_qfl=QualityFocalloss
loss_dfl=DistributionFocalloss

```




mmdet/ops/project.py


断点顺序：
1. `tools/train.py` 
   - 11行
   - 125行 建立 model



`mmdet/models/detectors/single_stage.py`

- 27行 SingleStageDetector 初始化

`mmdet/models/builder.py`  15行 build函数



1. `mmdet/models/anchor_heads/gfl_head.py` 
  - 53行 GFLHead类初始化
  - 67行 self.loss_qfl和loss_dfl初始化 
3. `tools/train.py` 142行 train_detector()函数



3. `mmdet/apis/train.py`166行 runner.run()函数

`mmdet/datasets/samplers/group_sampler.py` GroupSampler类
  - 48行  __len__ 函数
  -  

`mmdet/apis/train.py`  （主要训练的函数）
  - 59行 batch_processor()函数 计算loss

`mmdet/core/fp16/decorators.py` 
  - 49行 auto_fp16_wrapper函数 暂时不知道什么用 

`mmdet/models/detectors/single_stage.py`

  -  71行 SingleStageDetector类的forward_train函数，有提取特征等操作

`mmdet/core/fp16/decorators.py` 
  - 127行 force_fp32函数 暂时不知道什么用 

`mmdet/models/anchor_heads/gfl_head.py`

  - 216行 进入GFLHead类 loss函数
  - 243行 GFLHead类 loss函数中 计算losses_qfl,losses_bbox, losses_dfl,avg_factor

`mmdet/core/utils/misc.py` 22行 multi_apply函数



`mmdet/ops/conv_module.py` ConvModule 类

5. `mmdet/models/anchor_heads/gfl_head.py`
  - 149行  loss_single()函数
  - 190行   loss_single()函数中的loss_dfl
6. `mmdet/models/losses/gfocal_loss.py` 
  - 108行 DistributionFocalLoss类的forward函数
  - 42行 distribution_focal_loss()函数
  - 75行 QualityFocalLoss类的forward函数
  - 16行 quality_focal_loss()函数 

## 实验的整理

### 准备

**1.Trackit**（主要使用，数据集直接放在固态）
电脑：cv@supermicro
代码路径：`/mnt/data/tracking/wu_m/TracKit`或`/home/cv/wu/TracKit`
conda环境：Trackit

**2. BottleneckTransformers**（botnet的代码参考）
电脑：gby-wm@docker
代码路径：`/root/wu/github/BottleneckTransformers`
conda环境：pytracking2

**3. 电脑：gby-wm@docker**（备用）
代码路径：`/root/wu/TracKit`
conda环境：Trackit2

**4. 电脑：gby-wm@docker**（备用）
代码路径：`/root/wu/wyc/SOTS`
conda环境：Trackit1

### 代码的整理

#### 第一次修改，调试成功
**lib/models/connect.py**
- 添加`from .dcn.deform_conv import DeformConv`

#### 第二次修改，插入botnet

**lib/models/ocean.py**

- 修改Ocean_类，加入mhsa和single的特征提取

**lib/models/models.py**
- Ocean类添加self.feature_mhsa和self.feature_single

**lib/models/backbones.py**
- 添加ResNet_MHSA, ResNet_single类
- Resnet50类中6改为5

**lib/models/modules.py**
- 添加MHSA，Botteneck_MHSA, Resnet_plus_MHSA, Resnet_plus_single类

## 调试
### 调试BottleneckTransformers得出resnet和botnet基础模型
botnet

![image-20210222124147532](..\img\chapter03\3_3.png)

botnet结构
```
self.query:  1*1, 512,s1
self.key: 1*1, 512,s1
self.value: 1*1, 512, s1
self.rel_h: 1,512,1,14
self.rel_w: 1,512,14,1

x: 2,512,14,14   n_batch,c,w,h
q=self.query(x).view(2,512,-1)   [2,512,14,14]-->[2,512,196]
k=self.key(x).view(2,512,-1)   [2,512,14,14]-->[2,512,196]
v=self.value(x).view(2,512,-1)   [2,512,14,14]-->[2,512,196]

content=q*k   [2,196,196]
position=(self.rel_h+self.rel_w).view(1,C,-1).permute(0,2,1)
          [1,512,14,14]           [1,512,196]    [1,196,512]
position=position*q   [2,196,196]
energy=content+position
attention: [2,196,196]

out: v*attention(0,2,1)  [2,512,196]
out=out.view   [2,512,14,14]
```

### Ocean中backbone结构

![image-20210222150528993](..\img\chapter03\3_2.png)

### 类siam_tracker属性：
siam
Args:
align=True,arch=ocean,dataset=vot2018,epoch_test=false,online=false

resume='snapshot/oceanV' video=none

Info:TRT:FALSE. Arch:ocean. Dataset:vot2018.  Epoch_test:false

Info:TRT:FALSE. Arch:ocean. Dataset:vot2018.  Epoch_test:false,online:false

Siam_info.align=true

self.stride=8

Self.align=true

self.online=false

Self.trt=false

### 实验

#### 第一次实验，制作base

日志

`Ocean_2021-02-02-00-14`:第一次训练，训练了10个小时，完成了15%，batch使用的32

`Ocean_2021-02-02-10-12`：测试，读取checkpoint3，其他没变

`Ocean_2021-02-02-10-17`：测试，读取checkpoint3，batch使用64



`Ocean_2021-02-02-10-25`：读取原始预训练网络，batch使用64，训了25分钟



`Ocean_2021-02-02-10-52`：读取原始预训练网络，batch使用32，和第一次的配置一样，现在一直在训练

查看日志：
`~/wu/TracKit/logs/Ocean# tensorboard --logdir=./Ocean_2021-02-02-10-52 --port=5901`



几个训练数据集的大小：
vid:82G
det:18G
coco: 56G
got10k: 67G
y2b 144G

lasot 494G

![img](..\img\chapter03\3_1.png)

Ocean base训练完后
把online和OA分支都去掉了做一个base，三张卡训20个epoch训了45个小时，损失函数是上面那样的。训完后用checkpoint_e20.pth在OTB2015上测的success是0.672，precision是0.898，下一步准备把backbone换vit再试

测一下5-20个epoch的结果

checkpoint_e20的结果存放：`/root/wu/TracKit/result/OTB2015/Oceanbase_checkpoint_e20`
OTB2015 success：0.672  precision:0.898

Oceanbase_checkpoint_e19
OTB2015 success：0.672  precision:0.897

| checkpoint                      | success | precision |
| ------------------------------- | ------- | --------- |
| Oceanbase_checkpoint_e20        | 0.672   | 0.898     |
| Oceanbase_checkpoint_e20(align) | 0.669   | 0.894     |
| Oceanbase_checkpoint_e19        | 0.672   | 0.897     |
| Oceanbase_checkpoint_e18        | 0.668   | 0.896     |
| Oceanbase_checkpoint_e17        | 0.661   | 0.882     |
| Oceanbase_checkpoint_e16        | 0.668   | 0.894     |
| Oceanbase_checkpoint_e15        | 0.662   | 0.886     |
| Oceanbase_checkpoint_e14        | 0.670   | 0.896     |
| Oceanbase_checkpoint_e13        | 0.662   | 0.881     |
| Oceanbase_checkpoint_e12        | 0.667   | 0.889     |
| Oceanbase_checkpoint_e11        | 0.649   | 0.862     |
| Oceanbase_checkpoint_e10        | 0.634   | 0.850     |
| Oceanbase_checkpoint_e9         | 0.622   | 0.839     |
| Oceanbase_checkpoint_e8         | 0.633   | 0.850     |
| Oceanbase_checkpoint_e7         | 0.628   | 0.845     |
| Oceanbase_checkpoint_e6         | 0.609   | 0.834     |
| Oceanbase_checkpoint_e5         | 0.565   | 0.788     |

使用作者提供的OceanV

| checkpoint   | success | precision |
| ------------ | ------- | --------- |
| OceanV       | 0.637   | 0.856     |
| OceanV_align | 0.643   | 0.886     |



test：
```
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e20.pth --dataset OTB2015 --align False
```

`python lib/eval_toolkit/bin/eval.py --dataset_dir dataset/OTB2015 --dataset OTB2015 --tracker_result_dir result/OTB2015 --trackers Ocean`

我们把trackit跑通后，由于需要把数据放到2T的固态硬盘上，所以对docker做了一次commit，commit前主盘的容量272，占比66%，commit后容量292占比70%。足足多了20G，

这里，不得不考虑直接将trackit放到主盘来做
以后，当commit前尽量将无用的主盘数据删干净
尽量每个conda环境单独做docker

python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e17.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e17
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e16.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e16
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e15.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e15
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e14.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e14
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e13.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e13
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e12.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e12
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e11.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e11
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e10.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e10
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e9.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e9
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e8.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e8
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e7.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e7
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e6.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e6
python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e5.pth --dataset OTB2015 --align False
mv ~/wu/TracKit/result/OTB2015/Ocean result/OTB2015/Oceanbase_checkpoint_e5



#### 第二次实验，加入botnet看效果

### 调试的一些操作

for i in trainable_params[0]['params']:
    print(i)

test=[filter(lambda x: x.requires_grad,model.features_mhsa_z.features_mhsa.parameters())]

test
[<filter object at 0x...118c32350>]
for i in test[0]:
    print(i)  打印出一堆参数



model.features.features.parameters()
<generator object Module.parameters at 0x7fc118bc9a50>
for i in model.features.features.parameters():
    print(i) 打印出一堆参数



for i in model.features.features.parameters():
    print(i.requires_grad)



for name,param in model.connect_model.named_parameters():

  if param.requires_grad==True:

​    print(name)

代码执行流程：

```python
#self.features = ResNet50(used_layers=[3], online=online)  
class ResNet50(nn.Module):
    def __init__(self, used_layers=[2, 3, 4], online=False):
        super(ResNet50, self).__init__()
        self.features = ResNet_plus2(Bottleneck, [3, 4, 6, 3], used_layers=used_layers, online=online)

    def forward(self, x, online=False,templ=None):
        if not online:
            x_stages, x = self.features(x, online=online,templ=templ)
            return x_stages, x
        else:
            x = self.features(x, online=online)
            return x
```

```python
class ResNet_plus2(nn.Module):
    def __init__(self, block, layers, used_layers, online=False):
        self.inplanes = 64
        super(ResNet_plus2, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=0,  # 3
                               bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)

        self.feature_size = 128 * block.expansion
        self.used_layers = used_layers
        self.layer3_use = True if 3 in used_layers else False
        self.layer4_use = True if 4 in used_layers else False

        if self.layer3_use:
            if online:
                self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2, update=True)
                self.layeronline = self._make_layer(block, 256, layers[2], stride=2)
            else:
                self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)

            self.feature_size = (256 + 128) * block.expansion
        else:
            self.layer3 = lambda x: x  # identity

        if self.layer4_use:
            self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)  # 7x7, 3x3
            self.feature_size = 512 * block.expansion
        else:
            self.layer4 = lambda x: x  # identity

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, update=False):
        downsample = None
        dd = dilation
        if stride != 1 or self.inplanes != planes * block.expansion:
            if stride == 1 and dilation == 1:
                downsample = nn.Sequential(
                    nn.Conv2d(self.inplanes, planes * block.expansion,
                              kernel_size=1, stride=stride, bias=False),
                    nn.BatchNorm2d(planes * block.expansion),
                )
            else:
                if dilation > 1:
                    dd = dilation // 2
                    padding = dd
                else:
                    dd = 1
                    padding = 0
                downsample = nn.Sequential(
                    nn.Conv2d(self.inplanes, planes * block.expansion,
                              kernel_size=3, stride=stride, bias=False,
                              padding=padding, dilation=dd),
                    nn.BatchNorm2d(planes * block.expansion),
                )

        layers = []
        layers.append(block(self.inplanes, planes, stride=stride,
                            downsample=downsample, dilation=dilation))
        self.inplanes = planes * block.expansion
        if blocks!=6:
            for i in range(1, blocks):
                layers.append(block(self.inplanes, planes, dilation=dilation))
        else:
            for i in range(1, blocks-1):
                layers.append(block(self.inplanes, planes, dilation=dilation))
            #layers.append(block(self.inplanes, planes, dilation=dilation,mhsa=True))
            layers.append(block(self.inplanes, planes, dilation=dilation))

        if update: self.inplanes = int(self.inplanes / 2)  # for online
        return nn.Sequential(*layers)

    def forward(self, x, online=False):
        x = self.conv1(x)
        x = self.bn1(x)
        x_ = self.relu(x)
        x = self.maxpool(x_)

        p1 = self.layer1(x)
        p2 = self.layer2(p1)

        if online: return self.layeronline(p2)
        p3 = self.layer3(p2)

        return [x_, p1, p2], p3
```

```python
class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1, mhsa=False,templ=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        padding = 2 - stride
        if downsample is not None and dilation > 1:
            dilation = dilation // 2
            padding = dilation

        assert stride == 1 or dilation == 1, \
            "stride and dilation must have one equals to zero at least"

        if dilation > 1:
            padding = dilation
        if not mhsa:
            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
                               padding=padding, bias=False, dilation=dilation)
        else:
            self.conv2 = nn.Sequential(
                    MHSA(planes)
                    # MHSA(planes, width=4, height=4), # for CIFAR10
                )
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * 4)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x, templ=None):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        if templ is None:
            out = self.conv2(out)
        else:
            out = self.conv2(out,templ=templ)


        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual

        out = self.relu(out)

        return out
```

```python
# self.neck = AdjustLayer(in_channels=1024, out_channels=256)
class AdjustLayer(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(AdjustLayer, self).__init__()
        self.downsample = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_channels),
            )

    def forward(self, x, crop=False):
        x_ori = self.downsample(x)
        if x_ori.size(3) < 20 and crop:
            l = 4
            r = -4
            xf = x_ori[:, :, l:r, l:r]

        if not crop:
            return x_ori
        else:
            return x_ori, xf
```

```python
# self.connect_model = box_tower(inchannels=256, outchannels=256, towernum=4)
class box_tower(nn.Module):
    """
    box tower for FCOS reg
    """
    def __init__(self, inchannels=512, outchannels=256, towernum=1):
        super(box_tower, self).__init__()
        tower = []
        cls_tower = []
        # encode backbone
        self.cls_encode = matrix(in_channels=inchannels, out_channels=outchannels)
        self.reg_encode = matrix(in_channels=inchannels, out_channels=outchannels)
        self.cls_dw = GroupDW(in_channels=inchannels)
        self.reg_dw = GroupDW(in_channels=inchannels)

        # box pred head
        for i in range(towernum):
            if i == 0:
                tower.append(nn.Conv2d(outchannels, outchannels, kernel_size=3, stride=1, padding=1))
            else:
                tower.append(nn.Conv2d(outchannels, outchannels, kernel_size=3, stride=1, padding=1))

            tower.append(nn.BatchNorm2d(outchannels))
            tower.append(nn.ReLU())

        # cls tower
        for i in range(towernum):
            if i == 0:
                cls_tower.append(nn.Conv2d(outchannels, outchannels, kernel_size=3, stride=1, padding=1))
            else:
                cls_tower.append(nn.Conv2d(outchannels, outchannels, kernel_size=3, stride=1, padding=1))

            cls_tower.append(nn.BatchNorm2d(outchannels))
            cls_tower.append(nn.ReLU())

        self.add_module('bbox_tower', nn.Sequential(*tower))
        self.add_module('cls_tower', nn.Sequential(*cls_tower))


        # reg head
        self.bbox_pred = nn.Conv2d(outchannels, 4, kernel_size=3, stride=1, padding=1)
        self.cls_pred = nn.Conv2d(outchannels, 1, kernel_size=3, stride=1, padding=1)

        # adjust scale
        self.adjust = nn.Parameter(0.1 * torch.ones(1))
        self.bias = nn.Parameter(torch.Tensor(1.0 * torch.ones(1, 4, 1, 1)).cuda())

    def forward(self, search, kernal, update=None):
        # encode first
        if update is None:
            cls_z, cls_x = self.cls_encode(kernal, search)   # [z11, z12, z13]
        else:
            cls_z, cls_x = self.cls_encode(update, search)  # [z11, z12, z13]

        reg_z, reg_x = self.reg_encode(kernal, search)  # [x11, x12, x13]

        # cls and reg DW
        cls_dw = self.cls_dw(cls_z, cls_x)
        reg_dw = self.reg_dw(reg_z, reg_x)
        x_reg = self.bbox_tower(reg_dw)
        x = self.adjust * self.bbox_pred(x_reg) + self.bias
        x = torch.exp(x)

        # cls tower
        c = self.cls_tower(cls_dw)
        cls = 0.1 * self.cls_pred(c)

        return x, cls, cls_dw, x_reg
```

```python
class matrix(nn.Module):
    """
    encode backbone feature
    """
    def __init__(self, in_channels, out_channels):
        super(matrix, self).__init__()

        # same size (11)
        self.matrix11_k = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )
        self.matrix11_s = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )

        # h/2, w
        self.matrix12_k = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, kernel_size=3, bias=False, dilation=(2, 1)),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )
        self.matrix12_s = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, kernel_size=3, bias=False, dilation=(2, 1)),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )

        # w/2, h
        self.matrix21_k = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, kernel_size=3, bias=False, dilation=(1, 2)),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )
        self.matrix21_s = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, kernel_size=3, bias=False, dilation=(1, 2)),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )

    def forward(self, z, x):
        z11 = self.matrix11_k(z)
        x11 = self.matrix11_s(x)

        z12 = self.matrix12_k(z)
        x12 = self.matrix12_s(x)

        z21 = self.matrix21_k(z)
        x21 = self.matrix21_s(x)

        return [z11, z12, z21], [x11, x12, x21]
```

```python
class GroupDW(nn.Module):
    """
    encode backbone feature
    """
    def __init__(self, in_channels=256):
        super(GroupDW, self).__init__()
        self.weight = nn.Parameter(torch.ones(3))

    def forward(self, z, x):
        z11, z12, z21 = z
        x11, x12, x21 = x

        re11 = xcorr_depthwise(x11, z11)
        re12 = xcorr_depthwise(x12, z12)
        re21 = xcorr_depthwise(x21, z21)
        re = [re11, re12, re21]
        
        # weight
        weight = F.softmax(self.weight, 0)

        s = 0
        for i in range(3):
            s += weight[i] * re[i]

        return s
```

日志：

Ocean_2021-02-21-21-23



#### 第三次实验，加入nonlocal看效果

### 训练日志

Ocean_2021-02-28-11-04：前14个epoch，

Ocean_2020-02-28-20-50：后36个epoch，

Ocean_2021-03-01-20-27：后30个epoch，因为分类分支的学习率在之前一直没变化，所以在这里修改了分类分支的学习率

训练时冻结主干网和回归分支

训练结果：一直不收敛，原因可能是回归分支冻结，损失函数本来很小。或者是在原始框图上增加了一个W模块。在OTB20125实验结果增加了0.001

为了在测试时并行跑，修改了部分程序



## 所做修改：
参考代码：
**1.Non-local_pytorch**
电脑：cv@supermicro
代码路径：`/home/cv/wu/wyc/Non_Local/Non-local_pytorch`
conda环境：pytracking



1. tracking/train_ocean.py增加了build_opt_lr_nonlocal函数，修改模型的冻结层数，学习率以及优化参数。
2. testing/test_ocean.py 增加了--result输入参数，在测试时把结果保存在哪个路径下，用于并行测试多个模型
3. testing/test_ocean.py 读取模型打开
4. lib/models/connect.py增加并完善了_NonlocalBlockND类
5. lib/models/connect.py在box_tower中增加self.non_local模型并修改了forward函数
5. experiments/train/Ocean.yaml
    start_epoch: 0或20
    end_epoch:50
    LR.KWARGS.start_lr: 0.01
    LR.KWARGS.end_lr: 0.00001
    数据集，用YTB,VID,GOT10K
    UNFIX_EPOCH: 20

test.sh示例：
```
CUDA_VISIBLE_DEVICES=1 python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e25.pth --dataset OTB2015 --align
 False --result result1
 mv ~/wu/TracKit/result1/OTB2015/Ocean result1/OTB2015/Oceanolocal_checkpoint_e25
```







实验结果

使用nonlocal

| checkpoint                 | success | precision |
| -------------------------- | ------- | --------- |
| Oceanolocal_checkpoint_e2  | 0.666   | 0.889     |
| Oceanolocal_checkpoint_e3  | 0.673   | 0.900     |
| Oceanolocal_checkpoint_e4  | 0.666   | 0.888     |
| Oceanolocal_checkpoint_e5  | 0.669   | 0.893     |
| Oceanolocal_checkpoint_e6  | 0.667   | 0.891     |
| Oceanolocal_checkpoint_e7  | 0.671   | 0.897     |
| Oceanolocal_checkpoint_e8  | 0.669   | 0.895     |
| Oceanolocal_checkpoint_e9  | 0.666   | 0.887     |
| Oceanolocal_checkpoint_e10 | 0.665   | 0.888     |
| Oceanolocal_checkpoint_e11 | 0.671   | 0.897     |
| Oceanolocal_checkpoint_e12 | 0.671   | 0.897     |
| Oceanolocal_checkpoint_e13 | 0.666   | 0.888     |
| Oceanolocal_checkpoint_e14 | 0.669   | 0.892     |
| Oceanolocal_checkpoint_e15 |         |           |
| Oceanolocal_checkpoint_e16 |         |           |
| Oceanolocal_checkpoint_e17 |         |           |
| Oceanolocal_checkpoint_e18 |         |           |
| Oceanolocal_checkpoint_e19 |         |           |
| Oceanolocal_checkpoint_e20 |         |           |
| Oceanolocal_checkpoint_e21 | 0.669   | 0.894     |
| Oceanolocal_checkpoint_e22 | 0.666   | 0.888     |
| Oceanolocal_checkpoint_e23 | 0.662   | 0.883     |
| Oceanolocal_checkpoint_e24 | 0.665   | 0.887     |
| Oceanolocal_checkpoint_e25 | 0.664   | 0.886     |
| Oceanolocal_checkpoint_e26 | 0.668   | 0.892     |
| Oceanolocal_checkpoint_e27 | 0.671   | 0.897     |
| Oceanolocal_checkpoint_e28 | 0.658   | 0.878     |
| Oceanolocal_checkpoint_e29 | 0.673   | 0.898     |
| Oceanolocal_checkpoint_e30 | 0.666   | 0.890     |
| Oceanolocal_checkpoint_e31 | 0.671   | 0.896     |
| Oceanolocal_checkpoint_e32 | 0.673   | 0.898     |
| Oceanolocal_checkpoint_e33 | 0.668   | 0.891     |
| Oceanolocal_checkpoint_e34 | 0.659   | 0.882     |
| Oceanolocal_checkpoint_e35 | 0.666   | 0.888     |
| Oceanolocal_checkpoint_e36 | 0.672   | 0.896     |
| Oceanolocal_checkpoint_e37 | 0.672   | 0.898     |
| Oceanolocal_checkpoint_e38 | 0.666   | 0.892     |
| Oceanolocal_checkpoint_e39 | 0.669   | 0.893     |
| Oceanolocal_checkpoint_e40 | 0.668   | 0.892     |
| Oceanolocal_checkpoint_e41 | 0.672   | 0.897     |
| Oceanolocal_checkpoint_e42 | 0.658   | 0.880     |
| Oceanolocal_checkpoint_e43 | 0.671   | 0.896     |
| Oceanolocal_checkpoint_e44 | 0.670   | 0.894     |
| Oceanolocal_checkpoint_e45 | 0.672   | 0.898     |
| Oceanolocal_checkpoint_e46 | 0.665   | 0.887     |
| Oceanolocal_checkpoint_e47 | 0.673   | 0.900     |
| Oceanolocal_checkpoint_e48 | 0.670   | 0.894     |
| Oceanolocal_checkpoint_e49 | 0.664   | 0.886     |
| Oceanolocal_checkpoint_e50 | 0.673   | 0.900     |
|                            |         |           |


第四次实验，加入centerness看效果

### 训练日志

Ocean_2021-03-14-23-12：训练了18个epoch

训练时冻结了主干网和分类分支，在第15个epoch打开回归分支微调


## 所做修改：
参考代码：
**1.Siamcar**
电脑：cv@supermicro
代码路径：`/home/cv/wu/github/SiamCAR`
conda环境：pytracking

**2.nanodet**
电脑：cv@supermicro
代码路径：`/home/cv/wu/github/nanodet`
conda环境：nanodet

1. lib/core/function.py 增加了cen_losses变量
2. lib/models/connect.py  在模型中添加了centerness结构
3. lib/models/ocean.py  添加cen_pred
4. lib/tracker/ocean.py 修改net.track的返回值，重新计算pscore
5. tracking/train_ocean.py 修改build_opt_lr_cen
5. experiments/train/Ocean.yaml
    start_epoch: 0
    end_epoch:20
    LR.KWARGS.start_lr: 0.005
    LR.KWARGS.end_lr: 0.0005
    数据集，用YTB,VID,GOT10K,coco，det
    UNFIX_EPOCH: 15



| checkpoint                 | success | precision |
| -------------------------- | ------- | --------- |
| Oceanolocal_checkpoint_e2  | 0.658   | 0.874     |
| Oceanolocal_checkpoint_e3  | 0.655   | 0.870     |
| Oceanolocal_checkpoint_e4  | 0.652   | 0.867     |
| Oceanolocal_checkpoint_e5  | 0.658   | 0.878     |
| Oceanolocal_checkpoint_e6  | 0.657   | 0.876     |
| Oceanolocal_checkpoint_e7  | 0.661   | 0.883     |
| Oceanolocal_checkpoint_e8  | 0.661   | 0.879     |
| Oceanolocal_checkpoint_e9  | 0.666   | 0.889     |
| Oceanolocal_checkpoint_e10 | 0.659   | 0.876     |
| Oceanolocal_checkpoint_e11 | 0.661   | 0.882     |
| Oceanolocal_checkpoint_e12 | 0.660   | 0.879     |
| Oceanolocal_checkpoint_e13 | 0.664   | 0.886     |
| Oceanolocal_checkpoint_e14 | 0.653   | 0.881     |
| Oceanolocal_checkpoint_e15 | 0.654        | 0.871 |
| Oceanolocal_checkpoint_e16 | 0.661 | 0.880 |





test.sh示例：
```
CUDA_VISIBLE_DEVICES=1 python tracking/test_ocean.py --arch Ocean --resume snapshot/checkpoint_e25.pth --dataset OTB2015 --align
 False --result result1
 mv ~/wu/TracKit/result1/OTB2015/Ocean result1/OTB2015/Oceanolocal_checkpoint_e25
```