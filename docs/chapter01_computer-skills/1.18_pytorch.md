# Pytorch

torch.where()

**torch.index_select()**

np.where()

np.argwhere()



**在PyTorch中Tensor的查找和筛选例子**

https://www.w3xue.com/exp/article/20198/50849.html

https://blog.csdn.net/tfcy694/article/details/85332953



unsqueze()



# torch.clamp()



# torch.permute()和np.transpose()





# OpenCV warpAffine

https://www.freesion.com/article/2891863848/





# torch——通过复制扩展维度

label.shape=(n, 28, 28)
label = torch.repeat_interleave(label.unsqueeze(dim=1), repeats=5, dim=1)  # (n, 8, 56, 56)
label.shape=(n, 5, 28, 28)

https://www.cnblogs.com/haifwu/p/12814760.html





## backbone、head、neck等深度学习中的术语解释

我们在阅读文章的时候，经常看到backbone head  neck 这一类的术语，但是我们可能并不知道是什么意思，这篇文章就是对这些术语进行解释：

1.backbone：翻译为主干网络的意思，既然说是主干网络，就代表其是网络的一部分，那么是哪部分呢？翻译的很好，主干部分，哈哈哈哈，文字游戏了哈。这个主干网络大多时候指的是提取特征的网络，其作用就是提取图片中的信息，共后面的网络使用。这些网络经常使用的是resnet VGG等，而不是我们自己设计的网络，因为这些网络已经证明了在分类等问题上的特征提取能力是很强的。在用这些网络作为backbone的时候，都是直接加载官方已经训练好的模型参数，后面接着我们自己的网络。让网络的这两个部分同时进行训练，因为加载的backbone模型已经具有提取特征的能力了，在我们的训练过程中，会对他进行微调，使得其更适合于我们自己的任务。

2.head：head是获取网络输出内容的网络，利用之前提取的特征，head利用这些特征，做出预测。

3.neck:是放在backbone和head之间的，是为了更好的利用backbone提取的特征

4.bottleneck:瓶颈的意思，通常指的是网网络输入的数据维度和输出的维度不同，输出的维度比输入的小了许多，就像脖子一样，变细了。经常设置的参数 bottle_num=256，指的是网络输出的数据的维度是256 ，可是输入进来的可能是1024维度的。

5.GAP：在设计的网络中经常能够看到gap这个层，我之前不知道是干啥的，后了解了，就是Global Average Pool全局平均池化，就是将某个通道的特征取平均值，经常使用AdaptativeAvgpoold(1),在pytorch中，这个代表自适应性全局平均池化，说人话就是将某个通道的特征取平均值。

self.gap = nn.AdaptiveAvgPool2d(1)

6.Embedding: 深度学习方法都是利用使用线性和非线性转换对复杂的数据进行自动特征抽取，并将特征表示为“向量”（vector），这一过程一般也称为“嵌入”（embedding）

7.用于预训练的任务被称为前置/代理任务(pretext task)，用于微调的任务被称为下游任务(downstream task)

8.temperature parameters 在论文中经常能看到这个温度参数的身影，那么他都有什么用处呢？比如经常看到下面这样的式子：



里面的beta就是temperature parameter，他在运算的时候起到什么作用呢？是这样的，他可以起到平滑softmax输出结果的作用，举例子如下：

import torch
x = torch.tensor([1.0,2.0,3.0])
y = torch.softmax(x,0)
print(y)

x1 = x / 2  # beta 为2
y = torch.softmax(x1,0)
print(y)

x2 = x/0.5  # beta 为0.5
y = torch.softmax(x2,0)
print(y)
输出结果如下：

tensor([0.0900, 0.2447, 0.6652])
tensor([0.1863, 0.3072, 0.5065])
tensor([0.0159, 0.1173, 0.8668])
当beta>1的时候，可以将输出结果变得平滑，当beta<1的时候，可以让输出结果变得差异更大一下，更尖锐一些。如果beta比较大，则分类的crossentropy损失会很大，可以在不同的迭代次数里，使用不同的beta数值，有点类似于学习率的效果。

 

9.热身Warm up。Warm up指的是用一个小的学习率先训练几个epoch，这是因为网络的参数是随机初始化的，一开始就采用较大的学习率容易数值不稳定。

 

10 end to end  在论文中经常能遇到end to end这样的描述，那么到底什么是端到端呢？其实就是给了一个输入，我们就给出一个输出，不管其中的过程多么复杂，但只要给了一个输入，机会对应一个输出。比如分类问题，你输入了一张图片，肯呢个网络有特征提取，全链接分类，概率计算什么的，但是跳出算法问题，单从结果来看，就是给了一张输入，输出了一个预测结果。End-To-End的方案，即输入一张图，输出最终想要的结果，算法细节和学习过程全部丢给了神经网络。

11 domain adaptation 和domain generalization 域适应和域泛化

域适应中，常见的设置是源域D_S完全已知，目标域D_T有或无标签。域适应方法试着将源域知识迁移到目标域。第二种场景可以视为domain generalization域泛化。这种更常见因为将模型应用到完全未知的领域，正因为没有见过，所以没有任何模型更新和微调。这种泛化问题就是一种开集问题，由于所需预测类别较多，所以比较头疼 
————————————————
版权声明：本文为CSDN博主「Tchunren」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/t20134297/article/details/105745566



# 汇总|目标检测中的数据增强、backbone、head、neck、损失函数

## 一、数据增强方式

1. random erase
2. CutOut
3. MixUp
4. CutMix
5. 色彩、对比度增强
6. 旋转、裁剪

![img](https://pic4.zhimg.com/80/v2-9885575f68d82492730de6497137f89f_720w.jpg)

**解决数据不均衡：**

- Focal loss
- hard negative example mining
- OHEM
- S-OHEM
- GHM（较大关注easy和正常hard样本，较少关注outliners）
- PISA

![img](https://pic4.zhimg.com/80/v2-672ae604cbc8607cccf8848353fcc717_720w.jpg)

## 二、常用backbone

1. VGG
2. ResNet（ResNet18，50，100）
3. ResNeXt
4. DenseNet
5. SqueezeNet
6. Darknet（Darknet19,53）
7. MobileNet
8. ShuffleNet
9. DetNet
10. DetNAS
11. SpineNet
12. EfficientNet（EfficientNet-B0/B7）
13. CSPResNeXt50
14. CSPDarknet53

![img](https://pic3.zhimg.com/80/v2-a8a32c801c062e1f186ebfe11cb3d062_720w.jpg)

## 三、常用Head

**Dense Prediction (one-stage):**

1. RPN
2. SSD
3. YOLO
4. RetinaNet
5. (anchor based)
6. CornerNet
7. CenterNet
8. MatrixNet
9. FCOS(anchor free)

**Sparse Prediction (two-stage):**

1. Faster R-CNN
2. R-FCN
3. Mask RCNN (anchor based)
4. RepPoints(anchor free)

## 四、常用neck

**Additional blocks:**

1. SPP
2. ASPP
3. RFB
4. SAM

**Path-aggregation blocks:**

1. FPN
2. PAN
3. NAS-FPN
4. Fully-connected FPN
5. BiFPN
6. ASFF
7. SFAM
8. NAS-FPN

![img](https://pic3.zhimg.com/80/v2-79a5b356441273d4ae7832825973b736_720w.jpg)

## 五、Skip-connections

1. Residual connections
2. Weighted residual connections
3. Multi-input weighted residual connections
4. Cross stage partial connections (CSP)

## 六、常用激活函数和loss

**激活函数：**

- ReLU
- LReLU
- PReLU
- ReLU6
- Scaled Exponential Linear Unit (SELU)
- Swish
- hard-Swish
- Mish

**loss：**

- MSE
- Smooth L1
- Balanced L1
- KL Loss
- GHM loss
- IoU Loss
- Bounded IoU Loss
- GIoU Loss
- CIoU Loss
- DIoU Loss

## 七、正则化和BN方式

**正则化：**

- DropOut
- DropPath
- Spatial DropOut
- DropBlock

**BN：**

- Batch Normalization (BN)
- Cross-GPU Batch Normalization (CGBN or SyncBN)
- Filter Response Normalization (FRN)
- Cross-Iteration Batch Normalization (CBN)

![img](https://pic4.zhimg.com/80/v2-cb5d90e88dee1ba3d90279f971730f7f_720w.jpg)



## 八、训练技巧

- Label Smoothing
- Warm Up

**往期干货资源：**

[汇总 | 国内最全的3D视觉学习资源，涉及计算机视觉、SLAM、三维重建、点云处理、姿态估计、深度估计、3D检测、自动驾驶、深度学习（3D+2D）、图像处理、立体视觉、结构光等方向！](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/xyGndcupuK1Zzmv1AJA5CQ)

[汇总 | 3D目标检测（基于点云、双目、单目）](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/mp/appmsgalbum%3Faction%3Dgetalbum%26album_id%3D1433712471084466177%26__biz%3DMzU1MjY4MTA1MQ%3D%3D%23wechat_redirect)

[汇总 | 6D姿态估计算法（基于点云、单目、投票方式）](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/mp/appmsgalbum%3Faction%3Dgetalbum%26album_id%3D1433707278687109122%26__biz%3DMzU1MjY4MTA1MQ%3D%3D%23wechat_redirect)

[汇总 | 三维重建算法实战（单目重建、立体视觉、多视图几何）](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/mp/appmsgalbum%3Faction%3Dgetalbum%26album_id%3D1433700656199860224%26__biz%3DMzU1MjY4MTA1MQ%3D%3D%23wechat_redirect)

[汇总 | 3D点云后处理算法（匹配、检索、滤波、识别）](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/mp/appmsgalbum%3Faction%3Dgetalbum%26album_id%3D1329868938683187201%26__biz%3DMzU1MjY4MTA1MQ%3D%3D%23wechat_redirect)

[汇总 | SLAM算法（视觉里程计、后端优化、回环检测）](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/mp/appmsgalbum%3Faction%3Dgetalbum%26album_id%3D1329870989060308993%26__biz%3DMzU1MjY4MTA1MQ%3D%3D%23wechat_redirect)

[汇总 | 深度学习&自动驾驶前沿算法研究（检测、分割、多传感器融合）](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/mp/appmsgalbum%3Faction%3Dgetalbum%26album_id%3D1329874254812512256%26__biz%3DMzU1MjY4MTA1MQ%3D%3D%23wechat_redirect)

[汇总 | 相机标定算法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/mp/appmsgalbum%3Faction%3Dgetalbum%26album_id%3D1319739406642937857%26__biz%3DMzU1MjY4MTA1MQ%3D%3D%23wechat_redirect)

[汇总 | 事件相机原理](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/mp/appmsgalbum%3Faction%3Dgetalbum%26album_id%3D1329866645053210625%26__biz%3DMzU1MjY4MTA1MQ%3D%3D%23wechat_redirect)

[汇总 | 结构光经典算法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/mp/appmsgalbum%3Faction%3Dgetalbum%26album_id%3D1319744372111671296%26__biz%3DMzU1MjY4MTA1MQ%3D%3D%23wechat_redirect)

[汇总 | 缺陷检测常用算法与实战技巧](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/mp/appmsgalbum%3Faction%3Dgetalbum%26album_id%3D1433687238369705986%26__biz%3DMzU1MjY4MTA1MQ%3D%3D%23wechat_redirect)